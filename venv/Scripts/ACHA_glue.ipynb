{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files from base directory copied to merged_output directory.\n",
      "\n",
      "Files in Merged Output Directory (First 20 files):\n",
      "Org_FHAB_FHAB_1000_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1000_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1003_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1003_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1004_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1004_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1005_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1005_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1006_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1006_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1007_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1007_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1008_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1008_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1009_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1009_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1011_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1011_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1012_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1012_20241013T1345_RST.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define directories and list the files for each directory\n",
    "# Note: You should run this part in your local Jupyter environment where the directories exist\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths for the two directories\n",
    "base_dir = '/mnt/d/Doc_To_Date/20241024/20241013'\n",
    "output_dir = '/mnt/d/Doc_To_Date/20241024/merged_output'\n",
    "\n",
    "# Step 1.1: Create output directory if it doesn't exist and copy all base files into it\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Copy all files from base_dir to output_dir\n",
    "for file_name in os.listdir(base_dir):\n",
    "    base_file_path = os.path.join(base_dir, file_name)\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    shutil.copy(base_file_path, output_file_path)\n",
    "\n",
    "print(\"All files from base directory copied to merged_output directory.\")\n",
    "\n",
    "# Step 2: List files in the merged_output directory\n",
    "merged_files = os.listdir(output_dir)\n",
    "\n",
    "print(\"\\nFiles in Merged Output Directory (First 20 files):\")\n",
    "print(\"\\n\".join(merged_files[:20]))  # Print first 20 files for readability\n",
    "\n",
    "# We will stop here for now as per the current step requested.\n",
    "# Next steps can be implemented after verification of the copied files.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T10:42:13.856295Z",
     "start_time": "2024-10-24T10:41:11.044398200Z"
    }
   },
   "id": "57f433fabaf2672d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzed File Name Patterns (First 10 files):\n",
      "File: Org_FHAB_FHAB_1000_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1000, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1000_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1000, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1003_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1003, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1003_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1003, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1004_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1004, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1004_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1004, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1005_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1005, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1005_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1005, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1006_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1006, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1006_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1006, Suffix: 20241013T1345_RST\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Analyze file name patterns\n",
    "import re\n",
    "\n",
    "file_patterns = []\n",
    "\n",
    "# Regular expression to capture the different parts of the file name\n",
    "file_pattern_regex = re.compile(r'^(.*?_.*?)_(.*?)_(.*?)_(.*?)\\.txt$')\n",
    "\n",
    "for file_name in merged_files:\n",
    "    match = file_pattern_regex.match(file_name)\n",
    "    if match:\n",
    "        prefix, location, timestamp, suffix = match.groups()\n",
    "        file_patterns.append({\n",
    "            'file_name': file_name,\n",
    "            'prefix': prefix,\n",
    "            'location': location,\n",
    "            'timestamp': timestamp,\n",
    "            'suffix': suffix\n",
    "        })\n",
    "\n",
    "print(\"\\nAnalyzed File Name Patterns (First 10 files):\")\n",
    "for pattern in file_patterns[:10]:\n",
    "    print(f\"File: {pattern['file_name']} - Prefix: {pattern['prefix']}, Location: {pattern['location']}, Timestamp: {pattern['timestamp']}, Suffix: {pattern['suffix']}\")\n",
    "\n",
    "# We will stop here for now as per the current step requested.\n",
    "# Next steps can be implemented after verification of the file name patterns.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T10:53:47.549256700Z",
     "start_time": "2024-10-24T10:53:47.471517900Z"
    }
   },
   "id": "6c4dcf8b94db532c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files from base directory copied to merged_output directory.\n",
      "\n",
      "Files in Merged Output Directory (First 20 files):\n",
      "Org_FHAB_FHAB_1000_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1000_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1003_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1003_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1004_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1004_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1005_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1005_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1006_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1006_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1007_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1007_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1008_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1008_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1009_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1009_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1011_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1011_20241013T1345_RST.txt\n",
      "Org_FHAB_FHAB_1012_20241013T1345.txt\n",
      "Org_FHAB_FHAB_1012_20241013T1345_RST.txt\n",
      "\n",
      "Analyzed File Name Patterns (First 10 files):\n",
      "File: Org_FHAB_FHAB_1000_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1000, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1000_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1000, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1003_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1003, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1003_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1003, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1004_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1004, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1004_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1004, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1005_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1005, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1005_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1005, Suffix: 20241013T1345_RST\n",
      "File: Org_FHAB_FHAB_1006_20241013T1345.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1006, Suffix: 20241013T1345\n",
      "File: Org_FHAB_FHAB_1006_20241013T1345_RST.txt - Prefix: Org_FHAB, Location: FHAB, Timestamp: 1006, Suffix: 20241013T1345_RST\n",
      "\n",
      "Matched Files in Update Directory (First 10 files):\n",
      "Org_FHAB_FHAB_1000_20241018T0905.txt\n",
      "Org_FHAB_FHAB_1000_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_1003_20241018T0905.txt\n",
      "Org_FHAB_FHAB_1003_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_1004_20241018T0905.txt\n",
      "Org_FHAB_FHAB_1004_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_1005_20241018T0905.txt\n",
      "Org_FHAB_FHAB_1005_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_1006_20241018T0905.txt\n",
      "Org_FHAB_FHAB_1006_20241018T0905_RST.txt\n",
      "\n",
      "Unmatched Files in Update Directory (First 10 files):\n",
      "Org_FHAB_FHAB_2237_20241018T0905.txt\n",
      "Org_FHAB_FHAB_2237_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_2265_20241018T0905.txt\n",
      "Org_FHAB_FHAB_2265_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_2266_20241018T0905.txt\n",
      "Org_FHAB_FHAB_2266_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_2274_20241018T0905.txt\n",
      "Org_FHAB_FHAB_2274_20241018T0905_RST.txt\n",
      "Org_FHAB_FHAB_2328_20241018T0905.txt\n",
      "Org_FHAB_FHAB_2328_20241018T0905_RST.txt\n",
      "\n",
      "Total number of matched files: 1334\n",
      "Total number of unmatched files: 82\n",
      "\n",
      "Unmatched files copied to merged_output directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22817/560510864.py:97: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_base = pd.read_csv(base_file_path, delimiter='|', header=None, encoding='latin1')\n",
      "/tmp/ipykernel_22817/560510864.py:97: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_base = pd.read_csv(base_file_path, delimiter='|', header=None, encoding='latin1')\n",
      "/tmp/ipykernel_22817/560510864.py:97: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_base = pd.read_csv(base_file_path, delimiter='|', header=None, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "  Совпавшие файлы объединены и обновлены в каталоге merged_output.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define directories and list the files for each directory\n",
    "# Note: You should run this part in your local Jupyter environment where the directories exist\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Define the paths for the two directories\n",
    "base_dir = '/mnt/d/Doc_To_Date/20241024/20241013'\n",
    "update_dir = '/mnt/d/Doc_To_Date/20241024/20241018'\n",
    "output_dir = '/mnt/d/Doc_To_Date/20241024/merged_output'\n",
    "\n",
    "# Step 1.1: Create output directory if it doesn't exist and clear it if it already exists\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # Remove all contents of the output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Copy all files from base_dir to output_dir\n",
    "for file_name in os.listdir(base_dir):\n",
    "    base_file_path = os.path.join(base_dir, file_name)\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    shutil.copy(base_file_path, output_file_path)\n",
    "\n",
    "print(\"All files from base directory copied to merged_output directory.\")\n",
    "\n",
    "# Step 2: List files in the merged_output directory\n",
    "merged_files = os.listdir(output_dir)\n",
    "\n",
    "print(\"\\nFiles in Merged Output Directory (First 20 files):\")\n",
    "print(\"\\n\".join(merged_files[:20]))  # Print first 20 files for readability\n",
    "\n",
    "# Step 3: Analyze file name patterns in merged_output\n",
    "file_patterns = []\n",
    "\n",
    "# Regular expression to capture the different parts of the file name\n",
    "file_pattern_regex = re.compile(r'^(.*?_.*?)_(.*?)_(.*?)_(.*?)\\.txt$')\n",
    "\n",
    "for file_name in merged_files:\n",
    "    match = file_pattern_regex.match(file_name)\n",
    "    if match:\n",
    "        prefix, location, timestamp, suffix = match.groups()\n",
    "        file_patterns.append({\n",
    "            'file_name': file_name,\n",
    "            'prefix': prefix,\n",
    "            'location': location,\n",
    "            'timestamp': timestamp,\n",
    "            'suffix': suffix\n",
    "        })\n",
    "\n",
    "print(\"\\nAnalyzed File Name Patterns (First 10 files):\")\n",
    "for pattern in file_patterns[:10]:\n",
    "    print(f\"File: {pattern['file_name']} - Prefix: {pattern['prefix']}, Location: {pattern['location']}, Timestamp: {pattern['timestamp']}, Suffix: {pattern['suffix']}\")\n",
    "\n",
    "# Step 4: List and analyze files in update_dir to find matches and non-matches\n",
    "update_files = os.listdir(update_dir)\n",
    "matched_files = []\n",
    "unmatched_files = []\n",
    "\n",
    "for file_name in update_files:\n",
    "    match = file_pattern_regex.match(file_name)\n",
    "    if match:\n",
    "        prefix, location, timestamp, suffix = match.groups()\n",
    "        # Check if a file with the same prefix, location, and timestamp exists in the merged_output directory\n",
    "        if any(f['prefix'] == prefix and f['location'] == location and f['timestamp'] == timestamp for f in file_patterns):\n",
    "            matched_files.append(file_name)\n",
    "        else:\n",
    "            unmatched_files.append(file_name)\n",
    "\n",
    "print(\"\\nMatched Files in Update Directory (First 10 files):\")\n",
    "print(\"\\n\".join(matched_files[:10]))\n",
    "\n",
    "print(\"\\nUnmatched Files in Update Directory (First 10 files):\")\n",
    "print(\"\\n\".join(unmatched_files[:10]))\n",
    "\n",
    "# Print total number of matched and unmatched files\n",
    "print(f\"\\nTotal number of matched files: {len(matched_files)}\")\n",
    "print(f\"Total number of unmatched files: {len(unmatched_files)}\")\n",
    "\n",
    "# Step 5: Copy unmatched files from update_dir to merged_output\n",
    "for file_name in unmatched_files:\n",
    "    update_file_path = os.path.join(update_dir, file_name)\n",
    "    output_file_path = os.path.join(output_dir, file_name)\n",
    "    shutil.copy(update_file_path, output_file_path)\n",
    "\n",
    "print(\"\\nUnmatched files copied to merged_output directory.\")\n",
    "\n",
    "# Step 6: Merge matched files while preserving original formatting\n",
    "for file_name in matched_files:\n",
    "    # Define paths for base and update files\n",
    "    base_file_pattern = re.sub(r'_20241018T0905', '_20241013T1345', file_name)  # Convert update file to match base filename in merged_output\n",
    "    base_file_path = os.path.join(output_dir, base_file_pattern)\n",
    "    update_file_path = os.path.join(update_dir, file_name)\n",
    "\n",
    "    try:\n",
    "        # Read lines from both files\n",
    "        with open(base_file_path, 'r', encoding='cp1251') as base_file:  # Updated to use CP1251 encoding\n",
    "            base_lines = base_file.readlines()\n",
    "\n",
    "        with open(update_file_path, 'r', encoding='cp1251') as update_file:  # Updated to use CP1251 encoding\n",
    "            update_lines = update_file.readlines()\n",
    "\n",
    "        # Use a set to track lines in the base file to avoid duplicates\n",
    "        base_lines_set = set(base_lines)\n",
    "\n",
    "        # Add only unique lines from the update file to the base lines\n",
    "        merged_lines = base_lines[:]\n",
    "        for line in update_lines:\n",
    "            if line not in base_lines_set:\n",
    "                merged_lines.append(line)\n",
    "\n",
    "        # Write merged lines back to the base file\n",
    "        with open(base_file_path, 'w', encoding='cp1251') as merged_file:  # Updated to use CP1251 encoding\n",
    "            merged_file.writelines(merged_lines)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error in files: {base_file_path} or {update_file_path}, error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nMatched files merged and updated in merged_output directory.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:45.266222100Z",
     "start_time": "2024-10-24T11:10:31.538302Z"
    }
   },
   "id": "45e081897a4e6213"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
